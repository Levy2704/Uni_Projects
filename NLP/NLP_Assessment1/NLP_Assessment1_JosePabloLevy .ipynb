{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAqfOsv5keTE"
      },
      "source": [
        "# Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds9tijYishNI",
        "outputId": "844a47f9-fcaa-4c13-b478-330dc391b75c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "#Install Transformer\n",
        "#!pip install transformers\n",
        "#!pip install --upgrade diffusers transformers scipy\n",
        "#!pip install Xformers\n",
        "#!pip install sentencepiece\n",
        "#Warning restart kernel after installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2_rzXPfatfVu"
      },
      "outputs": [],
      "source": [
        "#Import libraries and pipeline\n",
        "from transformers import pipeline, AutoModelForCausalLM,Wav2Vec2ForCTC,Wav2Vec2Processor\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification,top_k_top_p_filtering\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, T5ForConditionalGeneration\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import librosa\n",
        "import torch\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9BPtWmYHkop"
      },
      "source": [
        "Activity 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr28JdVNvOy_",
        "outputId": "4ff2b2a7-c667-487e-fee1-3bbdb62b55f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'entity': 'B-PER', 'score': 0.9990451, 'index': 1, 'word': 'Levy', 'start': 0, 'end': 4}, {'entity': 'B-ORG', 'score': 0.9894198, 'index': 4, 'word': 'Torre', 'start': 16, 'end': 21}, {'entity': 'I-ORG', 'score': 0.95158076, 'index': 5, 'word': '##ns', 'start': 21, 'end': 23}, {'entity': 'B-LOC', 'score': 0.9988637, 'index': 9, 'word': 'Sydney', 'start': 37, 'end': 43}]\n"
          ]
        }
      ],
      "source": [
        "#define token and model\n",
        "tk = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "md = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "\n",
        "NER=pipeline(\"ner\", model=md, tokenizer=tk)\n",
        "example = \"Levy studies at Torrens and lives in Sydney\"\n",
        "results = NER(example)\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1tS3Bkix3Xf"
      },
      "source": [
        "\n",
        "Abbreviation\tDescription\n",
        "\n",
        "O\tOutside of a named entity\n",
        "\n",
        "*   O\tOutside of a named entity\n",
        "*   B-MIS\tBeginning of a miscellaneous entity right after another miscellaneous entity\n",
        "*   I-MIS\tMiscellaneous entity\n",
        "*   B-PER\tBeginning of a person’s name right after another person’s name\n",
        "*   I-PER\tPerson’s name\n",
        "*   B-ORG\tBeginning of an organization right after another organization\n",
        "*   B-LOC\tBeginning of a location right after another location\n",
        "*   I-LOC\tLocation\n",
        "\n",
        "Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. CoRR, abs/1810.04805. Retrieved from http://arxiv.org/abs/1810.04805\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC--01cIHpFY"
      },
      "source": [
        "Activity 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjyHRBPQv155",
        "outputId": "bd8bffdc-1d0f-4938-f964-22b07c53bd87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text 1: I absolutely loved the movie! The acting was fantastic.\n",
            "Sentiment: POSITIVE\n",
            "\n",
            "Text 2: The customer service was terrible. I had a horrible experience.\n",
            "Sentiment: NEGATIVE\n",
            "\n",
            "Text 3: The food at the restaurant was amazing. Highly recommended!\n",
            "Sentiment: POSITIVE\n",
            "\n",
            "Text 4: I found the book to be quite disappointing. Not worth the read.\n",
            "Sentiment: NEGATIVE\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Define model\n",
        "md=\"siebert/sentiment-roberta-large-english\"\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\",model=md)\n",
        "\n",
        "#Input\n",
        "texts = [\n",
        "    \"I absolutely loved the movie! The acting was fantastic.\",\n",
        "    \"The customer service was terrible. I had a horrible experience.\",\n",
        "    \"The food at the restaurant was amazing. Highly recommended!\",\n",
        "    \"I found the book to be quite disappointing. Not worth the read.\"\n",
        "]\n",
        "\n",
        "# Iterate over the texts and perform sentiment analysis\n",
        "for i, text in enumerate(texts):\n",
        "    sentiment_result = sentiment_analysis(text)\n",
        "    sentiment_label = sentiment_result[0]['label']\n",
        "\n",
        "    print(f\"Text {i+1}: {text}\")\n",
        "    print(f\"Sentiment: {sentiment_label}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuS3Ew18z-3Y"
      },
      "source": [
        "Hartmann, J., Heitmann, M., Siebert, C., & Schamp, C. (2023). More than a Feeling: Accuracy and Application of Sentiment Analysis. International Journal of Research in Marketing, 40(1), 75-87. https://doi.org/10.1016/j.ijresmar.2022.05.005"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1Z56AA-Hr78"
      },
      "source": [
        "Activity 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO1fmhJH1wYW",
        "outputId": "9f2de5cc-e4a6-414f-a7ed-ae4403b09bd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary: Coinbase Suddenly Surges After SEC Is Poised To Make A Game-Changing Decision. BNB, Ethereum also seen as possible candidates for Winklevoss Prize\n"
          ]
        }
      ],
      "source": [
        "# Load Pegasus tokenizer and model\n",
        "md = \"human-centered-summarization/financial-summarization-pegasus\"\n",
        "tk = PegasusTokenizer.from_pretrained(md)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(md)\n",
        "\n",
        "# Select URL\n",
        "url = \"https://www.forbes.com/sites/digital-assets/2023/07/04/coinbase-suddenly-surges-after-reports-suggest-sec-is-poised-to-make-a-game-changing-decision-that-could-play-havoc-with-the-price-of-bitcoin-ethereum-bnb-and-xrp/?sh=35998c011633\"\n",
        "\n",
        "# Fetch text from URL\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Tokenize text\n",
        "inputs = tk([text], truncation=True, padding='longest', max_length=512, return_tensors='pt')\n",
        "\n",
        "# Generate summary\n",
        "summary_ids = model.generate(inputs['input_ids'], num_beams=4, length_penalty=2.0, max_length=200, min_length=30, no_repeat_ngram_size=3, early_stopping=True)\n",
        "summary = tk.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print summary\n",
        "print(\"Summary:\", summary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx88ABDW1w7u"
      },
      "source": [
        "T. Passali, A. Gidiotis, E. Chatzikyriakidis and G. Tsoumakas. 2021. Towards Human-Centered Summarization: A Case Study on Financial News. In Proceedings of the First Workshop on Bridging Human-Computer Interaction and Natural Language Processing(pp. 21–27). Association for Computational Linguistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og6vZMApHvDI"
      },
      "source": [
        "Activity 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541,
          "referenced_widgets": [
            "b7fa51d96c214632adce838fccb06b27",
            "4ae632e5dfd84da096a0db04a8571363",
            "32ae32fd02e54e06a5a0a0092853c9ec",
            "8e69b86960ee426b9762587dfde57e15",
            "b21c0ad15e554883b26c4b5a727855ef",
            "fd6e52409a6e4c26a08ef82463057f6b",
            "3f462bc1fcbb419db7a929cbba41d2c9",
            "76abe87f6fee4a7993d4bc51f0c4d6ee",
            "be2fb65ab1134515a6531396aee2e772",
            "5d21d74e35de47fa87b9ac2a96a931d3",
            "5f4d682113034f51b13cb351dd73ed23"
          ]
        },
        "id": "uLBxeKaG5zh2",
        "outputId": "f729d1f2-9695-4d2e-ee6b-fe422d9afba1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An astrounaut riding a unicorn spaceship destroying the moon, the whole thing is a joke.\n",
            "\n",
            "A young man is walking down a street in a city, and suddenly he hears a scream and sees a gigantic man in black armor standing up. He sees the man, and he starts screaming for help. The man tries to stop the man by pulling out a gun, but the man falls through the wall and falls on his back. The man looks for the man, but he gets caught up in the fact that he has a gun and it is a good idea to go out and save him.\n",
            "\n",
            "The crowd around the crime scene is not happy with the man for it. The man gives the crowd a good reason to leave, but only for a moment. Then the crowd starts yelling, and he gives the crowd a bad reason to leave, too.\n",
            "\n",
            "The other half of the crowd is unhappy with the man for it, and asks if there can be a problem with him. He tells them to try and help him, but there is no help from anybody.\n",
            "\n",
            "The crowd tries to stop the man by forcing him to stand up, but he gets caught up in the fact that he has a gun and it is a good idea to go out and save him.\n",
            "\n",
            "In the beginning, the scene is really really weird. In the end, it's not all that interesting. The people are all being a lot more concerned with the man than the crowd. It's really not that big of a deal for them. The whole thing is just a joke.\n",
            "\n",
            "The next scene starts out pretty weird for a bunch of reasons, but one of them is this scene where the crowd starts screaming out loud, and the crowd is starting to realize the whole thing is actually a joke. It's not really a joke at all.\n",
            "\n",
            "The crowd begins to realize that nobody was going to be able to stop the man, and instead they are going to get in a shootout with the crowd. It's not like there's a big fight. It's a sort of a \"no\" situation.175\n",
            "\n",
            "Some of the other things that the crowd gets a good look at in the scene are that the crowd is having a bad time deciding which street to go to. A lot of them are having trouble deciding when to go to which street, and the same goes for the people who are screaming for help.\n",
            "\n",
            "The scene ends with a scene\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
            "```\n",
            "pip install accelerate\n",
            "```\n",
            ".\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7fa51d96c214632adce838fccb06b27",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the Text Generation pipeline\n",
        "md=\"gpt2\"\n",
        "text_generation = pipeline(\"text-generation\", model=md)\n",
        "\n",
        "# Starting prompt\n",
        "prompt = \"An astrounaut riding a unicorn spaceship destroying the moon\"\n",
        "\n",
        "# Generate text\n",
        "output = text_generation(prompt, max_length=500, do_sample=True, temperature=0.7)\n",
        "\n",
        "# Print the generated text\n",
        "generated_text = output[0]['generated_text']\n",
        "print(generated_text)\n",
        "\n",
        "#__________________________Extra_____________________________________________\n",
        "#Gives you the image of the prompt\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "device = \"cuda\"\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "image.save(\"astronaut_unicorn.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y5uK11N9BYd"
      },
      "source": [
        "Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-Resolution Image Synthesis With Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (pp. [10684-10695]). July."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_auw_l8Hxb5"
      },
      "source": [
        "Activity 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWVm2P8CgZ7X",
        "outputId": "02b2a11b-35ca-421b-b57f-8271527b7c35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To Cherish your visions. Cherish your ideals.\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "model_name = \"MaRiOrOsSi/t5-base-finetuned-question-answering\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "question = \"What is life?\"\n",
        "context = \"Life is to Cherish your visions. Cherish your ideals.Cherish the music that stirs in your heart, the beauty that forms in your mind, the loveliness that drapes your purest thoughts.For out of them will grow all delightful conditions, all heavenly environment, of these, if you but remain true to them, your world will at last be built.-James Allen\"\n",
        "input_text = f\"question: {question} context: {context}\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "output = model.generate(input_ids)\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlvuAjrXjVvp"
      },
      "source": [
        "Di Maio, C., & Nunziati, G. (Unknown). T5-based Question Answering Model [Model description]. Hugging Face. Retrieved July 3, 2023, from https://huggingface.co/MaRiOrOsSi/t5-base-finetuned-question-answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBpdRGLEH50a"
      },
      "source": [
        "Activity 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bawBBld-cgoS",
        "outputId": "6c38690c-0248-4f5f-e10a-93c7d98fc311"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English:  “To be creative means to be in love with life. You can be creative only if you love life enough that you want to enhance its beauty, you want to bring a little more music to it, a little more poetry to it, a little more dance to it.”-Osho.\n",
            "French:  « tre créatif, c'est être amoureux de la vie et ne peut être créatif que si l'on aime la vie suffisamment pour en améliorer la beauté, si l'on veut y apporter un peu plus de musique, un peu plus de poésie, un peu plus de danse.\n",
            "\n",
            "English:  We travel, initially, to lose ourselves; and we travel, next to find ourselves. We travel to open our hearts and eyes and learn more about the world than our newspapers will accommodate. We travel to bring what little we can, in our ignorance and knowledge, to those parts of the globe whose riches are differently dispersed. And we travel, in essence, to become young fools again- to slow time down and get taken in, and fall in love once more.― Pico Iyer\n",
            "French:  Nous voyageons, au départ, pour nous perdre; et nous voyageons, à la suite, pour nous retrouver. Nous voyageons pour ouvrir nos curs et nos yeux et en apprendre davantage sur le monde que nos journaux ne le permettront. Nous voyageons pour apporter ce peu que nous pouvons, dans notre ignorance et nos connaissances, dans les régions du globe dont les richesses sont différemment dispersées\n",
            "\n"
          ]
        }
      ],
      "source": [
        "translator = pipeline(\"translation_en_to_fr\")\n",
        "\n",
        "english_sentences = [\n",
        "    \"“To be creative means to be in love with life. You can be creative only if you love life enough that you want to enhance its beauty, you want to bring a little more music to it, a little more poetry to it, a little more dance to it.”-Osho.\",\n",
        "    \"We travel, initially, to lose ourselves; and we travel, next to find ourselves. We travel to open our hearts and eyes and learn more about the world than our newspapers will accommodate. We travel to bring what little we can, in our ignorance and knowledge, to those parts of the globe whose riches are differently dispersed. And we travel, in essence, to become young fools again- to slow time down and get taken in, and fall in love once more.― Pico Iyer\",\n",
        "]\n",
        "for sentence in english_sentences:\n",
        "    translated_text = translator(sentence)[0][\"translation_text\"]\n",
        "    print(\"English: \", sentence)\n",
        "    print(\"French: \", translated_text)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wXpyfQ8kXry"
      },
      "source": [
        "# Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONyxdEoOINQj"
      },
      "source": [
        "Activity 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztfRgEhkkcMZ",
        "outputId": "6f2b2083-e3c7-4244-bb15-0745664eed51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i have a personality that helps me be more efficient in my tasks.\n",
            "i have a job that helps me be more efficient in my tasks.\n",
            "i have a skill that helps me be more efficient in my tasks.\n",
            "i have a talent that helps me be more efficient in my tasks.\n",
            "i have a technique that helps me be more efficient in my tasks.\n"
          ]
        }
      ],
      "source": [
        "# Load the DistilBERT model for masked language modeling\n",
        "md = \"distilbert-base-uncased\"\n",
        "fill_mask = pipeline(\"fill-mask\", model=md)\n",
        "\n",
        "# Define the input text with a masked token\n",
        "input_text = \"I have a [MASK] that helps me be more efficient in my tasks.\"\n",
        "\n",
        "options = fill_mask(input_text, top_k=5)\n",
        "\n",
        "# Print the generated options\n",
        "for option in options:\n",
        "    print(option[\"sequence\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSSKJdTuIPr0"
      },
      "source": [
        "Activity 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_0hOoBClbJi",
        "outputId": "3209574b-fd73-440c-b3f9-68967a54cdce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Headline: Private equity is failing water companies again. Get these firms back on the stock market\n",
            "Sentiment: negative\n",
            "\n",
            "Headline: What equity markets got wrong about China\n",
            "Sentiment: neutral\n",
            "\n",
            "Headline: Treasury urged to launch campaign to boost stock market investment\n",
            "Sentiment: positive\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define model\n",
        "md= \"ProsusAI/finbert\"\n",
        "\n",
        "# Load the sentiment analysis pipeline\n",
        "sentiment_classifier = pipeline(\"sentiment-analysis\", model=md)\n",
        "\n",
        "headlines = [\n",
        "    \"Private equity is failing water companies again. Get these firms back on the stock market\",\n",
        "    \"What equity markets got wrong about China\",\n",
        "    \"Treasury urged to launch campaign to boost stock market investment\"\n",
        "]\n",
        "\n",
        "\n",
        "# Classify the sentiment of the headlines\n",
        "for headline in headlines:\n",
        "    result = sentiment_classifier(headline)\n",
        "    sentiment = result[0][\"label\"]\n",
        "    print(\"Headline:\", headline)\n",
        "    print(\"Sentiment:\", sentiment)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRJgFI_Yp_bS"
      },
      "source": [
        "Headlines retrieved from here\n",
        "\n",
        "*   https://www.afr.com/markets/equity-markets/what-equity-markets-got-wrong-about-china-20230705-p5dlud\n",
        "\n",
        "*   https://www.proactiveinvestors.com.au/companies/news/1019840/treasury-urged-to-launch-campaign-to-boost-stock-market-investment-1019840.html\n",
        "*   https://www.theguardian.com/business/nils-pratley-on-finance/2023/jul/05/the-best-way-to-save-thames-water-list-it-on-the-stock-market\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g82ZMnKISJM"
      },
      "source": [
        "Activity 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "lP2MgIbMrYmR",
        "outputId": "46bb4c10-401b-43b4-a3a8-a24e0f59ddb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> User:what is a dog?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DialoGPT: A big ol'pupper.\n",
            ">> User:do you like dogs?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DialoGPT: I like dogs.\n",
            ">> User:\u0004\n"
          ]
        },
        {
          "ename": "EOFError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-921a08eeffa5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnew_user_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">> User:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Encode the new user input, add the eos_token and return a tensor in PyTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'\\x04'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0;31m# EOF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEOFError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\", padding_side=\"left\")\n",
        "md = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "\n",
        "for step in range(5):\n",
        "    new_user_input = input(\">> User:\")\n",
        "\n",
        "    # Encode the new user input, add the eos_token and return a tensor in PyTorch\n",
        "    new_user_input_ids = tokenizer.encode(new_user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "\n",
        "    # Append the new user input tokens to the chat history\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "    # Generate a response while limiting the total chat history to 1000 tokens\n",
        "    chat_history_ids = md.generate(\n",
        "        bot_input_ids,\n",
        "        max_length=1000,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        temperature=0.8,  # Adjust the temperature for output accuracy\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    # Print the generated response\n",
        "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VxmutbQssgZ"
      },
      "source": [
        "Zhang, Y. (2019, November 1). DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation. arXiv.org. Retrieved July 5, 2023, from https://arxiv.org/abs/1911.00536\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvuTwygKIURQ"
      },
      "source": [
        "Activity 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98TfZdPYDsAt",
        "outputId": "83b288ea-12d4-4272-b06c-bb587b354d1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "THE FINAL EPISODE DETAILED THE PROBLEMS DURING PREPARATIONS FOR THE COUPLES WEDDING\n"
          ]
        }
      ],
      "source": [
        "#load model and tokenizer\n",
        "tokenizer = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "#Load any audio file of your choice\n",
        "# upload your audio file via left bar of colaboratory and use an appropriate path\n",
        "speech, rate = librosa.load(\"/content/common_voice.wav\",sr=16000)\n",
        "\n",
        "# Next, we tokenize the inputs and make sure to set our tensors to PyTorch objects instead\n",
        "input_values = tokenizer(speech, return_tensors = 'pt').input_values\n",
        "\n",
        "#Store logits (non-normalized predictions)\n",
        "logits = model(input_values).logits\n",
        "#Store predicted id's\n",
        "predicted_ids = torch.argmax(logits, dim =-1)\n",
        "#decode the audio to generate text\n",
        "transcriptions = tokenizer.decode(predicted_ids[0])\n",
        "print(transcriptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swWe4TVkvglf"
      },
      "source": [
        "https://commonvoice.mozilla.org/es/datasets"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "32ae32fd02e54e06a5a0a0092853c9ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76abe87f6fee4a7993d4bc51f0c4d6ee",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be2fb65ab1134515a6531396aee2e772",
            "value": 50
          }
        },
        "3f462bc1fcbb419db7a929cbba41d2c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ae632e5dfd84da096a0db04a8571363": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd6e52409a6e4c26a08ef82463057f6b",
            "placeholder": "​",
            "style": "IPY_MODEL_3f462bc1fcbb419db7a929cbba41d2c9",
            "value": "100%"
          }
        },
        "5d21d74e35de47fa87b9ac2a96a931d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f4d682113034f51b13cb351dd73ed23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76abe87f6fee4a7993d4bc51f0c4d6ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e69b86960ee426b9762587dfde57e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d21d74e35de47fa87b9ac2a96a931d3",
            "placeholder": "​",
            "style": "IPY_MODEL_5f4d682113034f51b13cb351dd73ed23",
            "value": " 50/50 [00:07&lt;00:00,  6.78it/s]"
          }
        },
        "b21c0ad15e554883b26c4b5a727855ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7fa51d96c214632adce838fccb06b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ae632e5dfd84da096a0db04a8571363",
              "IPY_MODEL_32ae32fd02e54e06a5a0a0092853c9ec",
              "IPY_MODEL_8e69b86960ee426b9762587dfde57e15"
            ],
            "layout": "IPY_MODEL_b21c0ad15e554883b26c4b5a727855ef"
          }
        },
        "be2fb65ab1134515a6531396aee2e772": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd6e52409a6e4c26a08ef82463057f6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
